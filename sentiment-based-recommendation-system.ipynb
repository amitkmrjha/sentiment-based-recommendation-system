{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Problem Statement***\n",
    "\n",
    "The e-commerce business is quite popular today. Here, you do not need to take orders by going to each customer. A company launches its website to sell the items to the end consumer, and customers can order the products that they require from the same website. Famous examples of such e-commerce companies are Amazon, Flipkart, Myntra, Paytm and Snapdeal.\n",
    "\n",
    "Suppose you are working as a Machine Learning Engineer in an e-commerce company named 'Ebuss'. Ebuss has captured a huge market share in many fields, and it sells the products in various categories such as household essentials, books, personal care products, medicines, cosmetic items, beauty products, electrical appliances, kitchen and dining products and health care products.\n",
    "\n",
    "With the advancement in technology, it is imperative for Ebuss to grow quickly in the e-commerce market to become a major leader in the market because it has to compete with the likes of Amazon, Flipkart, etc., which are already market leaders.\n",
    "\n",
    "As a senior ML Engineer, you are asked to build a model that will improve the recommendations given to the users given their past reviews and ratings. \n",
    "\n",
    " \n",
    "\n",
    "In order to do this, you planned to build a sentiment-based product recommendation system, which includes the following tasks.\n",
    "\n",
    "Data sourcing and sentiment analysis\n",
    "Building a recommendation system\n",
    "Improving the recommendations using the sentiment analysis model\n",
    "Deploying the end-to-end project with a user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4QFLCH2BVbf",
    "outputId": "b3e69890-cc46-4102-ecb5-661c42f20ec4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amit.kumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/amit.kumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amit.kumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/amit.kumar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer ,PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Ensure all required resources are present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "53cca964",
    "outputId": "0cdc71be-ae0c-43d8-80a9-f6df8db3e8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>name</th>\n",
       "      <th>reviews_date</th>\n",
       "      <th>reviews_didPurchase</th>\n",
       "      <th>reviews_doRecommend</th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>reviews_title</th>\n",
       "      <th>reviews_userCity</th>\n",
       "      <th>reviews_userProvince</th>\n",
       "      <th>reviews_username</th>\n",
       "      <th>user_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AV13O1A8GV-KLJ3akUyj</td>\n",
       "      <td>Universal Music</td>\n",
       "      <td>Movies, Music &amp; Books,Music,R&amp;b,Movies &amp; TV,Mo...</td>\n",
       "      <td>Universal Music Group / Cash Money</td>\n",
       "      <td>Pink Friday: Roman Reloaded Re-Up (w/dvd)</td>\n",
       "      <td>2012-11-30T06:21:45.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>i love this album. it's very good. more to the...</td>\n",
       "      <td>Just Awesome</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joshua</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>2017-07-09T00:00:00.000Z</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Good flavor. This review was collected as part...</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dorothy w</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>2017-07-09T00:00:00.000Z</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Good flavor.</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dorothy w</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>2016-01-06T00:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>I read through the reviews on here before look...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rebecca</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>2016-12-21T00:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>My husband bought this gel for us. The gel cau...</td>\n",
       "      <td>Irritation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>walker557</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id            brand  \\\n",
       "0  AV13O1A8GV-KLJ3akUyj  Universal Music   \n",
       "1  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "2  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "3  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "4  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "\n",
       "                                          categories  \\\n",
       "0  Movies, Music & Books,Music,R&b,Movies & TV,Mo...   \n",
       "1  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...   \n",
       "2  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...   \n",
       "3  Personal Care,Medicine Cabinet,Lubricant/Sperm...   \n",
       "4  Personal Care,Medicine Cabinet,Lubricant/Sperm...   \n",
       "\n",
       "                         manufacturer  \\\n",
       "0  Universal Music Group / Cash Money   \n",
       "1                            Lundberg   \n",
       "2                            Lundberg   \n",
       "3                                 K-Y   \n",
       "4                                 K-Y   \n",
       "\n",
       "                                         name              reviews_date  \\\n",
       "0   Pink Friday: Roman Reloaded Re-Up (w/dvd)  2012-11-30T06:21:45.000Z   \n",
       "1  Lundberg Organic Cinnamon Toast Rice Cakes  2017-07-09T00:00:00.000Z   \n",
       "2  Lundberg Organic Cinnamon Toast Rice Cakes  2017-07-09T00:00:00.000Z   \n",
       "3            K-Y Love Sensuality Pleasure Gel  2016-01-06T00:00:00.000Z   \n",
       "4            K-Y Love Sensuality Pleasure Gel  2016-12-21T00:00:00.000Z   \n",
       "\n",
       "  reviews_didPurchase reviews_doRecommend  reviews_rating  \\\n",
       "0                 NaN                 NaN               5   \n",
       "1                True                 NaN               5   \n",
       "2                True                 NaN               5   \n",
       "3               False               False               1   \n",
       "4               False               False               1   \n",
       "\n",
       "                                        reviews_text reviews_title  \\\n",
       "0  i love this album. it's very good. more to the...  Just Awesome   \n",
       "1  Good flavor. This review was collected as part...          Good   \n",
       "2                                       Good flavor.          Good   \n",
       "3  I read through the reviews on here before look...  Disappointed   \n",
       "4  My husband bought this gel for us. The gel cau...    Irritation   \n",
       "\n",
       "  reviews_userCity reviews_userProvince reviews_username user_sentiment  \n",
       "0      Los Angeles                  NaN           joshua       Positive  \n",
       "1              NaN                  NaN        dorothy w       Positive  \n",
       "2              NaN                  NaN        dorothy w       Positive  \n",
       "3              NaN                  NaN          rebecca       Negative  \n",
       "4              NaN                  NaN        walker557       Negative  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   id                    30000 non-null  object\n",
      " 1   brand                 30000 non-null  object\n",
      " 2   categories            30000 non-null  object\n",
      " 3   manufacturer          29859 non-null  object\n",
      " 4   name                  30000 non-null  object\n",
      " 5   reviews_date          29954 non-null  object\n",
      " 6   reviews_didPurchase   15932 non-null  object\n",
      " 7   reviews_doRecommend   27430 non-null  object\n",
      " 8   reviews_rating        30000 non-null  int64 \n",
      " 9   reviews_text          30000 non-null  object\n",
      " 10  reviews_title         29810 non-null  object\n",
      " 11  reviews_userCity      1929 non-null   object\n",
      " 12  reviews_userProvince  170 non-null    object\n",
      " 13  reviews_username      29937 non-null  object\n",
      " 14  user_sentiment        29999 non-null  object\n",
      "dtypes: int64(1), object(14)\n",
      "memory usage: 3.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "brand                       0\n",
       "categories                  0\n",
       "manufacturer              141\n",
       "name                        0\n",
       "reviews_date               46\n",
       "reviews_didPurchase     14068\n",
       "reviews_doRecommend      2570\n",
       "reviews_rating              0\n",
       "reviews_text                0\n",
       "reviews_title             190\n",
       "reviews_userCity        28071\n",
       "reviews_userProvince    29830\n",
       "reviews_username           63\n",
       "user_sentiment              1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('sample30.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to get an overview of the data\n",
    "print(\"Original DataFrame head:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display information about the DataFrame, including data types and non-null values\n",
    "print(\"\\nDataFrame info:\")\n",
    "display(df.info())\n",
    "\n",
    "# Display the number of missing values in each column\n",
    "print(\"\\nMissing values per column:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fed2be7a"
   },
   "source": [
    "### Data Cleaning and Pre-processing\n",
    "\n",
    "Based on the missing value analysis, we will decide on the appropriate strategy for handling missing values. This might involve imputation (replacing missing values with a calculated value like the mean, median, or mode) or removal (dropping rows or columns with missing values), depending on the extent and nature of the missing data.\n",
    "\n",
    "We will also drop columns that are not relevant for our analysis to simplify the dataset and improve performance. Finally, we will ensure all columns have the correct data types for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "_Kv5ftRqpkgb"
   },
   "outputs": [],
   "source": [
    "def clean_reviews_date(df, col='reviews_date'):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes a reviews_date column.\n",
    "\n",
    "    Steps:\n",
    "    1. Replace junk values (N/A, null, etc.)\n",
    "    2. Parse valid dates with pandas (fast)\n",
    "    3. For still-missing values, try dateutil parser (slower but flexible)\n",
    "    4. Return cleaned dataframe and log summary\n",
    "    \"\"\"\n",
    "    # Step 1: Replace common junk values with NA\n",
    "    junk_values = ['N/A', 'NA', 'na', 'null', 'None', 'NONE', 'Unknown', '', ' ']\n",
    "    df[col] = df[col].replace(junk_values, pd.NA)\n",
    "\n",
    "    # Step 2: First attempt with pandas (fast, flexible)\n",
    "    parsed = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    # Step 3: For rows still missing, try dateutil parser (slower but more powerful)\n",
    "    mask_missing = parsed.isna() & df[col].notna()\n",
    "    if mask_missing.sum() > 0:\n",
    "        def try_parse_date(x):\n",
    "            try:\n",
    "                return parser.parse(x, dayfirst=False, fuzzy=True)\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        parsed.loc[mask_missing] = df.loc[mask_missing, col].apply(try_parse_date)\n",
    "\n",
    "    # Step 4: Assign cleaned column back\n",
    "    df[col] = parsed\n",
    "\n",
    "    # Step 5: Log summary\n",
    "    total = len(df)\n",
    "    valid = df[col].notna().sum()\n",
    "    missing = df[col].isna().sum()\n",
    "    print(f\"[INFO] Cleaned '{col}': {valid}/{total} valid dates, {missing} missing ({missing/total:.2%})\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "2618b2e1",
    "outputId": "b3387cf9-7dd2-4b22-d0fe-5274ea588590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping irrelevant columns...\n",
      "Dropped columns: ['reviews_userCity', 'reviews_userProvince']\n",
      "[INFO] Cleaned 'reviews_date': 29946/30000 valid dates, 54 missing (0.18%)\n",
      "\n",
      "Cleaned and Pre-processed DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>name</th>\n",
       "      <th>reviews_date</th>\n",
       "      <th>reviews_didPurchase</th>\n",
       "      <th>reviews_doRecommend</th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>reviews_title</th>\n",
       "      <th>reviews_username</th>\n",
       "      <th>user_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AV13O1A8GV-KLJ3akUyj</td>\n",
       "      <td>Universal Music</td>\n",
       "      <td>Movies, Music &amp; Books,Music,R&amp;b,Movies &amp; TV,Mo...</td>\n",
       "      <td>Pink Friday: Roman Reloaded Re-Up (w/dvd)</td>\n",
       "      <td>2012-11-30 06:21:45+00:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>i love this album. it's very good. more to the...</td>\n",
       "      <td>Just Awesome</td>\n",
       "      <td>joshua</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>2017-07-09 00:00:00+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>Good flavor. This review was collected as part...</td>\n",
       "      <td>Good</td>\n",
       "      <td>dorothy w</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>2017-07-09 00:00:00+00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>Good flavor.</td>\n",
       "      <td>Good</td>\n",
       "      <td>dorothy w</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>2016-01-06 00:00:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>I read through the reviews on here before look...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>rebecca</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>2016-12-21 00:00:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>My husband bought this gel for us. The gel cau...</td>\n",
       "      <td>Irritation</td>\n",
       "      <td>walker557</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id            brand  \\\n",
       "0  AV13O1A8GV-KLJ3akUyj  Universal Music   \n",
       "1  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "2  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "3  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "4  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "\n",
       "                                          categories  \\\n",
       "0  Movies, Music & Books,Music,R&b,Movies & TV,Mo...   \n",
       "1  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...   \n",
       "2  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...   \n",
       "3  Personal Care,Medicine Cabinet,Lubricant/Sperm...   \n",
       "4  Personal Care,Medicine Cabinet,Lubricant/Sperm...   \n",
       "\n",
       "                                         name              reviews_date  \\\n",
       "0   Pink Friday: Roman Reloaded Re-Up (w/dvd) 2012-11-30 06:21:45+00:00   \n",
       "1  Lundberg Organic Cinnamon Toast Rice Cakes 2017-07-09 00:00:00+00:00   \n",
       "2  Lundberg Organic Cinnamon Toast Rice Cakes 2017-07-09 00:00:00+00:00   \n",
       "3            K-Y Love Sensuality Pleasure Gel 2016-01-06 00:00:00+00:00   \n",
       "4            K-Y Love Sensuality Pleasure Gel 2016-12-21 00:00:00+00:00   \n",
       "\n",
       "  reviews_didPurchase reviews_doRecommend  reviews_rating  \\\n",
       "0             Unknown                 Yes               5   \n",
       "1                True                 Yes               5   \n",
       "2                True                 Yes               5   \n",
       "3               False               False               1   \n",
       "4               False               False               1   \n",
       "\n",
       "                                        reviews_text reviews_title  \\\n",
       "0  i love this album. it's very good. more to the...  Just Awesome   \n",
       "1  Good flavor. This review was collected as part...          Good   \n",
       "2                                       Good flavor.          Good   \n",
       "3  I read through the reviews on here before look...  Disappointed   \n",
       "4  My husband bought this gel for us. The gel cau...    Irritation   \n",
       "\n",
       "  reviews_username user_sentiment  \n",
       "0           joshua       Positive  \n",
       "1        dorothy w       Positive  \n",
       "2        dorothy w       Positive  \n",
       "3          rebecca       Negative  \n",
       "4        walker557       Negative  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of handling missing values (replace with appropriate strategy based on analysis)\n",
    "\n",
    "print(\"\\nDropping irrelevant columns...\")\n",
    "df = df.drop(columns=['manufacturer'])   # redundant\n",
    "\n",
    "columns_to_drop = ['reviews_userCity', 'reviews_userProvince']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "#Use user_sentiment or reviews_rating to impute:\n",
    "#If sentiment = Positive (or rating ≥4) → fill \"Yes\".\n",
    "#If sentiment = Negative (or rating ≤2) → fill \"No\".\n",
    "#Neutral cases → \"Unknown\".\n",
    "#This keeps imputation consistent with actual review content.\n",
    "\n",
    "df['reviews_doRecommend'] = df.apply(\n",
    "    lambda x: 'Yes' if pd.isna(x['reviews_doRecommend']) and x['reviews_rating'] >= 4\n",
    "    else ('No' if pd.isna(x['reviews_doRecommend']) and x['reviews_rating'] <= 2\n",
    "    else (x['reviews_doRecommend'] if pd.notna(x['reviews_doRecommend']) else 'Unknown')),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# fill missing with empty string\n",
    "df['reviews_title'] = df['reviews_title'].fillna(\"\")\n",
    "\n",
    "# fill missing with Anonymous string\n",
    "df['reviews_username'] = df['reviews_username'].fillna(\"Anonymous\")\n",
    "\n",
    "\n",
    "#convert data type\n",
    "df = clean_reviews_date(df, 'reviews_date')  # convert to datetime\n",
    "df['reviews_rating'] = df['reviews_rating'].astype('int')  # ensure integer\n",
    "df['reviews_didPurchase'] = df['reviews_didPurchase'].fillna('Unknown').astype('category')\n",
    "df['reviews_doRecommend'] = df['reviews_doRecommend'].astype('category')\n",
    "df['user_sentiment'] = df['user_sentiment'].astype('category')\n",
    "\n",
    "# Display the first few rows of the cleaned and pre-processed DataFrame\n",
    "print(\"\\nCleaned and Pre-processed DataFrame head:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "1y8IcbTgsOmE"
   },
   "outputs": [],
   "source": [
    "# reviews_date (54 missing, <0.2%)\n",
    "# Very few → you don’t lose much if you drop them.\n",
    "# Since date is important for time-based analysis but not for core sentiment/recommendation we can drop those rows\n",
    "\n",
    "df = df[df['reviews_date'].notna()]\n",
    "\n",
    "df = df[df['user_sentiment'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "0IwVVQuRuL0e",
    "outputId": "eae6d35e-c22f-4b4a-dac1-bf7678e8367a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info after dropping columns:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29945 entries, 0 to 29999\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype              \n",
      "---  ------               --------------  -----              \n",
      " 0   id                   29945 non-null  object             \n",
      " 1   brand                29945 non-null  object             \n",
      " 2   categories           29945 non-null  object             \n",
      " 3   name                 29945 non-null  object             \n",
      " 4   reviews_date         29945 non-null  datetime64[ns, UTC]\n",
      " 5   reviews_didPurchase  29945 non-null  category           \n",
      " 6   reviews_doRecommend  29945 non-null  category           \n",
      " 7   reviews_rating       29945 non-null  int64              \n",
      " 8   reviews_text         29945 non-null  object             \n",
      " 9   reviews_title        29945 non-null  object             \n",
      " 10  reviews_username     29945 non-null  object             \n",
      " 11  user_sentiment       29945 non-null  category           \n",
      "dtypes: category(3), datetime64[ns, UTC](1), int64(1), object(7)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column after handling:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                     0\n",
       "brand                  0\n",
       "categories             0\n",
       "name                   0\n",
       "reviews_date           0\n",
       "reviews_didPurchase    0\n",
       "reviews_doRecommend    0\n",
       "reviews_rating         0\n",
       "reviews_text           0\n",
       "reviews_title          0\n",
       "reviews_username       0\n",
       "user_sentiment         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the DataFrame info after dropping columns\n",
    "print(\"\\nDataFrame info after dropping columns:\")\n",
    "display(df.info())\n",
    "\n",
    "# After handling missing values, display the updated missing value count\n",
    "print(\"Missing values per column after handling:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "OF6NwK13waiN",
    "outputId": "7ba3f9d6-c5ee-457e-8453-1796b67f81c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing to 'reviews_text' and 'reviews_title'...\n",
      "\n",
      "DataFrame head with preprocessed text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>reviews_text_preprocessed</th>\n",
       "      <th>reviews_title</th>\n",
       "      <th>reviews_title_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love this album. it's very good. more to the...</td>\n",
       "      <td>love album good hip hop side current pop sound...</td>\n",
       "      <td>Just Awesome</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good flavor. This review was collected as part...</td>\n",
       "      <td>good flavor review collected part promotion</td>\n",
       "      <td>Good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good flavor.</td>\n",
       "      <td>good flavor</td>\n",
       "      <td>Good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I read through the reviews on here before look...</td>\n",
       "      <td>read review looking buying one couple lubrican...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>disappointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My husband bought this gel for us. The gel cau...</td>\n",
       "      <td>husband bought gel u gel caused irritation fel...</td>\n",
       "      <td>Irritation</td>\n",
       "      <td>irritation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        reviews_text  \\\n",
       "0  i love this album. it's very good. more to the...   \n",
       "1  Good flavor. This review was collected as part...   \n",
       "2                                       Good flavor.   \n",
       "3  I read through the reviews on here before look...   \n",
       "4  My husband bought this gel for us. The gel cau...   \n",
       "\n",
       "                           reviews_text_preprocessed reviews_title  \\\n",
       "0  love album good hip hop side current pop sound...  Just Awesome   \n",
       "1        good flavor review collected part promotion          Good   \n",
       "2                                        good flavor          Good   \n",
       "3  read review looking buying one couple lubrican...  Disappointed   \n",
       "4  husband bought gel u gel caused irritation fel...    Irritation   \n",
       "\n",
       "  reviews_title_preprocessed  \n",
       "0                    awesome  \n",
       "1                       good  \n",
       "2                       good  \n",
       "3               disappointed  \n",
       "4                 irritation  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "try:\n",
    "    stopwords = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords = stopwords.words('english')\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer()\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses text data.\n",
    "\n",
    "    Steps:\n",
    "    1. Convert text to lowercase.\n",
    "    2. Remove punctuation.\n",
    "    3. Remove stop words.\n",
    "    4. Apply stemming or lemmatization (choose one).\n",
    "    \"\"\"\n",
    "    # 1. Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 3. Remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "    # 4. Apply lemmatization (you can switch to stemming if preferred)\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    # text = ' '.join([stemmer.stem(word) for word in text.split()]) # Uncomment for stemming\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the reviews_text and reviews_title columns\n",
    "print(\"Applying text preprocessing to 'reviews_text' and 'reviews_title'...\")\n",
    "df['reviews_text_preprocessed'] = df['reviews_text'].apply(preprocess_text)\n",
    "df['reviews_title_preprocessed'] = df['reviews_title'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows with the new preprocessed columns\n",
    "print(\"\\nDataFrame head with preprocessed text:\")\n",
    "display(df[['reviews_text', 'reviews_text_preprocessed', 'reviews_title', 'reviews_title_preprocessed']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "lH4ueCnc2kJc",
    "outputId": "e1f0c8a9-c50e-415c-f21b-11a2665eefef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets.\n",
      "X_train shape: (23956,)\n",
      "X_test shape: (5989,)\n",
      "y_train shape: (23956,)\n",
      "y_test shape: (5989,)\n",
      "\n",
      "Text data vectorized using TF-IDF.\n",
      "X_train_tfidf shape: (23956, 5000)\n",
      "X_test_tfidf shape: (5989, 5000)\n",
      "\n",
      "Handling class imbalance using SMOTE...\n",
      "Class imbalance handled using SMOTE.\n",
      "Resampled X_train shape: (42540, 5000)\n",
      "Resampled y_train shape: (42540,)\n",
      "Resampled y_train distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_sentiment\n",
       "Negative    21270\n",
       "Positive    21270\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Step 1: Split data into training and testing parts\n",
    "# We will use 'reviews_text_preprocessed' and 'user_sentiment' for our model\n",
    "X = df['reviews_text_preprocessed']\n",
    "y = df['user_sentiment']\n",
    "\n",
    "# Split data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data split into training and testing sets.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Step 2: Convert the text to features using TF-IDF vectorizer\n",
    "# Initialize TF-IDF vectorizer\n",
    "# max_features can be adjusted to limit the number of features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit the vectorizer on the training data and transform both training and testing data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"\\nText data vectorized using TF-IDF.\")\n",
    "print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
    "print(f\"X_test_tfidf shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Step 3: Handle class imbalance using SMOTE\n",
    "print(\"\\nHandling class imbalance using SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Class imbalance handled using SMOTE.\")\n",
    "print(f\"Resampled X_train shape: {X_train_resampled.shape}\")\n",
    "print(f\"Resampled y_train shape: {y_train_resampled.shape}\")\n",
    "print(\"Resampled y_train distribution:\")\n",
    "display(y_train_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "408acdf1",
    "outputId": "9a2730df-564b-4096-df5e-bb414c9f217e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:  33%|████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                        | 1/3 [00:00<00:00,  8.07it/s][Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    9.8s finished\n",
      "Models: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All models trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "# Step 3: Build and train models\n",
    "\n",
    "# Initialize the models\n",
    "log_reg = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear solver for smaller datasets\n",
    "rf_clf = RandomForestClassifier(random_state=42, verbose=1)\n",
    "nb_clf = MultinomialNB() # Suitable for text data with TF-IDF or Count Vectorizer\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Random Forest\": rf_clf,\n",
    "    \"Naive Bayes\": nb_clf\n",
    "}\n",
    "\n",
    "\n",
    "# Train each model\n",
    "print(\"Training models...\")\n",
    "for name, model in tqdm(models.items(), desc=\"Models\"):\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"\\nAll models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "16cb230b",
    "outputId": "d6b5bba0-218e-4c2f-8a52-cc3947de54e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models...\n",
      "Evaluating Logistic Regression...\n",
      "Logistic Regression evaluated.\n",
      "Evaluating Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest evaluated.\n",
      "Evaluating Naive Bayes...\n",
      "Naive Bayes evaluated.\n",
      "\n",
      "Model Performance Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.920521</td>\n",
       "      <td>0.980353</td>\n",
       "      <td>0.929095</td>\n",
       "      <td>0.954036</td>\n",
       "      <td>0.960981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.927701</td>\n",
       "      <td>0.950387</td>\n",
       "      <td>0.969156</td>\n",
       "      <td>0.959680</td>\n",
       "      <td>0.944603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.845717</td>\n",
       "      <td>0.955611</td>\n",
       "      <td>0.866466</td>\n",
       "      <td>0.908858</td>\n",
       "      <td>0.878445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall  F1-Score   ROC-AUC\n",
       "Logistic Regression  0.920521   0.980353  0.929095  0.954036  0.960981\n",
       "Random Forest        0.927701   0.950387  0.969156  0.959680  0.944603\n",
       "Naive Bayes          0.845717   0.955611  0.866466  0.908858  0.878445"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Evaluate models\n",
    "\n",
    "print(\"Evaluating models...\")\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_pred_proba = model.predict_proba(X_test_tfidf)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='Positive')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='Positive')\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='Positive')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 'N/A'\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC-AUC\": roc_auc\n",
    "    }\n",
    "    print(f\"{name} evaluated.\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "results_df = pd.DataFrame(results).T\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB0Mu7AbLN6m"
   },
   "source": [
    "**Naive Bayes**\n",
    "\n",
    "Lowest across all metrics → clearly the weakest.\n",
    "\n",
    "**Logistic Regression vs Random Forest**\n",
    "\n",
    "**Accuracy:** Random Forest slightly better (0.928 vs 0.920).\n",
    "\n",
    "**Precision:** Logistic Regression wins (0.980 vs 0.951).\n",
    "\n",
    "**Recall:** Random Forest wins big (0.969 vs 0.929).\n",
    "\n",
    "**F1-Score:** Random Forest wins (0.960 vs 0.954).\n",
    "\n",
    "**ROC-AUC:** Logistic Regression wins (0.961 vs 0.946).\n",
    "\n",
    "**So it’s a trade-off:**\n",
    "\n",
    "Logistic Regression → More conservative, higher precision (fewer false positives).\n",
    "\n",
    "Random Forest → More aggressive, higher recall & F1 (fewer false negatives).\n",
    "\n",
    "**Best Model Choice**\n",
    "\n",
    "If your system’s priority is catching as many positive reviews as possible (high recall, balanced F1), then → Random Forest.\n",
    "\n",
    "If you care more about precision (only recommend when very confident), then → Logistic Regression.\n",
    "\n",
    "For a sentiment-based product recommendation system, usually recall & F1 matter more — because you don’t want to miss positive sentiments that drive recommendations.\n",
    "\n",
    "**Best Overall Model = Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49b0ece2"
   },
   "source": [
    "# Build System\n",
    "\n",
    "Build and evaluate user-based and item-based recommendation systems on the dataset from \"/content/sample_data/sample30.csv\", select the best performing model, and provide reasons for the selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "395f9e22"
   },
   "source": [
    "## Prepare data for recommendation system\n",
    "\n",
    "Select the necessary columns and potentially preprocess them for building the recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9e2619f"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a new DataFrame with only the necessary columns for the recommendation system and display its head and info.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "9654d63c",
    "outputId": "38af0fed-74b6-44f9-fd0a-df22d6e0c633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_username</th>\n",
       "      <th>name</th>\n",
       "      <th>reviews_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joshua</td>\n",
       "      <td>Pink Friday: Roman Reloaded Re-Up (w/dvd)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dorothy w</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dorothy w</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rebecca</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walker557</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reviews_username                                        name  reviews_rating\n",
       "0           joshua   Pink Friday: Roman Reloaded Re-Up (w/dvd)               5\n",
       "1        dorothy w  Lundberg Organic Cinnamon Toast Rice Cakes               5\n",
       "2        dorothy w  Lundberg Organic Cinnamon Toast Rice Cakes               5\n",
       "3          rebecca            K-Y Love Sensuality Pleasure Gel               1\n",
       "4        walker557            K-Y Love Sensuality Pleasure Gel               1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviews DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29945 entries, 0 to 29999\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   reviews_username  29945 non-null  object\n",
      " 1   name              29945 non-null  object\n",
      " 2   reviews_rating    29945 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 935.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_df = df[['reviews_username', 'name', 'reviews_rating']]\n",
    "print(\"Reviews DataFrame head:\")\n",
    "display(reviews_df.head())\n",
    "print(\"\\nReviews DataFrame info:\")\n",
    "display(reviews_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3199991d"
   },
   "source": [
    "## Split data\n",
    "\n",
    "Divide the data into training and testing sets for evaluating the recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60994dae"
   },
   "source": [
    "**Reasoning**:\n",
    "The goal is to split the data into training and testing sets for the recommendation system. The previous subtask created the `reviews_df` DataFrame with relevant columns. This step will perform the split using `train_test_split`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8827843e",
    "outputId": "5a788859-03e9-4eba-e984-ba4abee0bb68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (23956, 3)\n",
      "Testing set shape: (5989, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the reviews_df DataFrame into training and testing sets\n",
    "train_df, test_df = train_test_split(reviews_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting DataFrames\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Testing set shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65ba37dd"
   },
   "source": [
    "## Build user-based collaborative filtering model\n",
    "\n",
    "Implement a user-based collaborative filtering recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3bd5aee"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a pivot table from the training data, calculate user similarity, and define a function for user-based recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "id": "1b947a7b",
    "outputId": "0c4be273-ae09-41b1-b059-6f859c084199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Matrix head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. Fire File Chest</th>\n",
       "      <th>100:Complete First Season (blu-Ray)</th>\n",
       "      <th>2017-2018 Brownline174 Duraflex 14-Month Planner 8 1/2 X 11 Black</th>\n",
       "      <th>2x Ultra Era with Oxi Booster, 50fl oz</th>\n",
       "      <th>42 Dual Drop Leaf Table with 2 Madrid Chairs\"</th>\n",
       "      <th>4C Grated Parmesan Cheese 100% Natural 8oz Shaker</th>\n",
       "      <th>Africa's Best No-Lye Dual Conditioning Relaxer System Super</th>\n",
       "      <th>Alberto VO5 Salon Series Smooth Plus Sleek Shampoo</th>\n",
       "      <th>Alex Cross (dvdvideo)</th>\n",
       "      <th>All,bran Complete Wheat Flakes, 18 Oz.</th>\n",
       "      <th>...</th>\n",
       "      <th>Walkers Stem Ginger Shortbread</th>\n",
       "      <th>Wallmount Server Cabinet (450mm, 9 RU)</th>\n",
       "      <th>Way Basics 3-Shelf Eco Narrow Bookcase Storage Shelf, Espresso - Formaldehyde Free - Lifetime Guarantee</th>\n",
       "      <th>WeatherTech 40647 14-15 Outlander Cargo Liners Behind 2nd Row, Black</th>\n",
       "      <th>Wedding Wishes Wedding Guest Book</th>\n",
       "      <th>Weleda Everon Lip Balm</th>\n",
       "      <th>Wilton Black Dots Standard Baking Cups</th>\n",
       "      <th>Windex Original Glass Cleaner Refill 67.6oz (2 Liter)</th>\n",
       "      <th>Yes To Carrots Nourishing Body Wash</th>\n",
       "      <th>Yes To Grapefruit Rejuvenating Body Wash</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_username</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00sab00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01impala</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02dakota</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02deuce</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0325home</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "name              0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. Fire File Chest  \\\n",
       "reviews_username                                                                  \n",
       "00sab00                                                         0.0               \n",
       "01impala                                                        0.0               \n",
       "02dakota                                                        0.0               \n",
       "02deuce                                                         0.0               \n",
       "0325home                                                        0.0               \n",
       "\n",
       "name              100:Complete First Season (blu-Ray)  \\\n",
       "reviews_username                                        \n",
       "00sab00                                           0.0   \n",
       "01impala                                          0.0   \n",
       "02dakota                                          0.0   \n",
       "02deuce                                           0.0   \n",
       "0325home                                          0.0   \n",
       "\n",
       "name              2017-2018 Brownline174 Duraflex 14-Month Planner 8 1/2 X 11 Black  \\\n",
       "reviews_username                                                                      \n",
       "00sab00                                                         0.0                   \n",
       "01impala                                                        0.0                   \n",
       "02dakota                                                        0.0                   \n",
       "02deuce                                                         0.0                   \n",
       "0325home                                                        0.0                   \n",
       "\n",
       "name              2x Ultra Era with Oxi Booster, 50fl oz  \\\n",
       "reviews_username                                           \n",
       "00sab00                                              0.0   \n",
       "01impala                                             0.0   \n",
       "02dakota                                             0.0   \n",
       "02deuce                                              0.0   \n",
       "0325home                                             0.0   \n",
       "\n",
       "name              42 Dual Drop Leaf Table with 2 Madrid Chairs\"  \\\n",
       "reviews_username                                                  \n",
       "00sab00                                                     0.0   \n",
       "01impala                                                    0.0   \n",
       "02dakota                                                    0.0   \n",
       "02deuce                                                     0.0   \n",
       "0325home                                                    0.0   \n",
       "\n",
       "name              4C Grated Parmesan Cheese 100% Natural 8oz Shaker  \\\n",
       "reviews_username                                                      \n",
       "00sab00                                                         0.0   \n",
       "01impala                                                        0.0   \n",
       "02dakota                                                        0.0   \n",
       "02deuce                                                         0.0   \n",
       "0325home                                                        0.0   \n",
       "\n",
       "name              Africa's Best No-Lye Dual Conditioning Relaxer System Super  \\\n",
       "reviews_username                                                                \n",
       "00sab00                                                         0.0             \n",
       "01impala                                                        0.0             \n",
       "02dakota                                                        0.0             \n",
       "02deuce                                                         0.0             \n",
       "0325home                                                        0.0             \n",
       "\n",
       "name              Alberto VO5 Salon Series Smooth Plus Sleek Shampoo  \\\n",
       "reviews_username                                                       \n",
       "00sab00                                                         0.0    \n",
       "01impala                                                        0.0    \n",
       "02dakota                                                        0.0    \n",
       "02deuce                                                         0.0    \n",
       "0325home                                                        0.0    \n",
       "\n",
       "name              Alex Cross (dvdvideo)  \\\n",
       "reviews_username                          \n",
       "00sab00                             0.0   \n",
       "01impala                            0.0   \n",
       "02dakota                            0.0   \n",
       "02deuce                             0.0   \n",
       "0325home                            0.0   \n",
       "\n",
       "name              All,bran Complete Wheat Flakes, 18 Oz.  ...  \\\n",
       "reviews_username                                          ...   \n",
       "00sab00                                              0.0  ...   \n",
       "01impala                                             0.0  ...   \n",
       "02dakota                                             0.0  ...   \n",
       "02deuce                                              0.0  ...   \n",
       "0325home                                             0.0  ...   \n",
       "\n",
       "name              Walkers Stem Ginger Shortbread  \\\n",
       "reviews_username                                   \n",
       "00sab00                                      0.0   \n",
       "01impala                                     0.0   \n",
       "02dakota                                     0.0   \n",
       "02deuce                                      0.0   \n",
       "0325home                                     0.0   \n",
       "\n",
       "name              Wallmount Server Cabinet (450mm, 9 RU)  \\\n",
       "reviews_username                                           \n",
       "00sab00                                              0.0   \n",
       "01impala                                             0.0   \n",
       "02dakota                                             0.0   \n",
       "02deuce                                              0.0   \n",
       "0325home                                             0.0   \n",
       "\n",
       "name              Way Basics 3-Shelf Eco Narrow Bookcase Storage Shelf, Espresso - Formaldehyde Free - Lifetime Guarantee  \\\n",
       "reviews_username                                                                                                            \n",
       "00sab00                                                         0.0                                                         \n",
       "01impala                                                        0.0                                                         \n",
       "02dakota                                                        0.0                                                         \n",
       "02deuce                                                         0.0                                                         \n",
       "0325home                                                        0.0                                                         \n",
       "\n",
       "name              WeatherTech 40647 14-15 Outlander Cargo Liners Behind 2nd Row, Black  \\\n",
       "reviews_username                                                                         \n",
       "00sab00                                                         0.0                      \n",
       "01impala                                                        0.0                      \n",
       "02dakota                                                        0.0                      \n",
       "02deuce                                                         0.0                      \n",
       "0325home                                                        0.0                      \n",
       "\n",
       "name              Wedding Wishes Wedding Guest Book  Weleda Everon Lip Balm  \\\n",
       "reviews_username                                                              \n",
       "00sab00                                         0.0                     0.0   \n",
       "01impala                                        0.0                     0.0   \n",
       "02dakota                                        0.0                     0.0   \n",
       "02deuce                                         0.0                     0.0   \n",
       "0325home                                        0.0                     0.0   \n",
       "\n",
       "name              Wilton Black Dots Standard Baking Cups  \\\n",
       "reviews_username                                           \n",
       "00sab00                                              0.0   \n",
       "01impala                                             0.0   \n",
       "02dakota                                             0.0   \n",
       "02deuce                                              0.0   \n",
       "0325home                                             0.0   \n",
       "\n",
       "name              Windex Original Glass Cleaner Refill 67.6oz (2 Liter)  \\\n",
       "reviews_username                                                          \n",
       "00sab00                                                         0.0       \n",
       "01impala                                                        0.0       \n",
       "02dakota                                                        0.0       \n",
       "02deuce                                                         0.0       \n",
       "0325home                                                        0.0       \n",
       "\n",
       "name              Yes To Carrots Nourishing Body Wash  \\\n",
       "reviews_username                                        \n",
       "00sab00                                           0.0   \n",
       "01impala                                          0.0   \n",
       "02dakota                                          0.0   \n",
       "02deuce                                           0.0   \n",
       "0325home                                          0.0   \n",
       "\n",
       "name              Yes To Grapefruit Rejuvenating Body Wash  \n",
       "reviews_username                                            \n",
       "00sab00                                                0.0  \n",
       "01impala                                               0.0  \n",
       "02dakota                                               0.0  \n",
       "02deuce                                                0.0  \n",
       "0325home                                               0.0  \n",
       "\n",
       "[5 rows x 263 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-Item Matrix shape: (20527, 263)\n",
      "\n",
      "User Similarity Matrix head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>reviews_username</th>\n",
       "      <th>00sab00</th>\n",
       "      <th>01impala</th>\n",
       "      <th>02dakota</th>\n",
       "      <th>02deuce</th>\n",
       "      <th>0325home</th>\n",
       "      <th>1.11E+24</th>\n",
       "      <th>1085</th>\n",
       "      <th>10ten</th>\n",
       "      <th>1234</th>\n",
       "      <th>1234567</th>\n",
       "      <th>...</th>\n",
       "      <th>zsazsa</th>\n",
       "      <th>zt313</th>\n",
       "      <th>zubb</th>\n",
       "      <th>zulaa118</th>\n",
       "      <th>zwithanx</th>\n",
       "      <th>zxcsdfd</th>\n",
       "      <th>zxjki</th>\n",
       "      <th>zyiah4</th>\n",
       "      <th>zzdiane</th>\n",
       "      <th>zzz1127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_username</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00sab00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01impala</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02dakota</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02deuce</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0325home</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "reviews_username  00sab00  01impala  02dakota  02deuce  0325home  1.11E+24  \\\n",
       "reviews_username                                                             \n",
       "00sab00               1.0       0.0       0.0      0.0       0.0       0.0   \n",
       "01impala              0.0       1.0       1.0      1.0       0.0       0.0   \n",
       "02dakota              0.0       1.0       1.0      1.0       0.0       0.0   \n",
       "02deuce               0.0       1.0       1.0      1.0       0.0       0.0   \n",
       "0325home              0.0       0.0       0.0      0.0       1.0       1.0   \n",
       "\n",
       "reviews_username  1085  10ten  1234  1234567  ...  zsazsa  zt313  zubb  \\\n",
       "reviews_username                              ...                        \n",
       "00sab00            0.0    0.0   0.0      0.0  ...     0.0    0.0   0.0   \n",
       "01impala           0.0    0.0   0.0      0.0  ...     0.0    1.0   0.0   \n",
       "02dakota           0.0    0.0   0.0      0.0  ...     0.0    1.0   0.0   \n",
       "02deuce            0.0    0.0   0.0      0.0  ...     0.0    1.0   0.0   \n",
       "0325home           0.0    0.0   0.0      1.0  ...     0.0    0.0   0.0   \n",
       "\n",
       "reviews_username  zulaa118  zwithanx  zxcsdfd  zxjki  zyiah4  zzdiane  zzz1127  \n",
       "reviews_username                                                                \n",
       "00sab00                0.0       0.0      0.0    0.0     0.0      0.0      0.0  \n",
       "01impala               0.0       0.0      0.0    0.0     0.0      0.0      0.0  \n",
       "02dakota               0.0       0.0      0.0    0.0     0.0      0.0      0.0  \n",
       "02deuce                0.0       0.0      0.0    0.0     0.0      0.0      0.0  \n",
       "0325home               0.0       1.0      1.0    1.0     1.0      0.0      1.0  \n",
       "\n",
       "[5 rows x 20527 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-based recommendation function defined.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Create a pivot table from the training data\n",
    "user_item_matrix = train_df.pivot_table(index='reviews_username', columns='name', values='reviews_rating').fillna(0)\n",
    "\n",
    "print(\"User-Item Matrix head:\")\n",
    "display(user_item_matrix.head())\n",
    "print(\"\\nUser-Item Matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "# 2. Calculate the pairwise cosine similarity between users\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "\n",
    "print(\"\\nUser Similarity Matrix head:\")\n",
    "display(user_similarity_df.head())\n",
    "\n",
    "# 3. Define a function for user-based recommendations\n",
    "def user_based_recommendations(user_id, user_item_matrix, user_similarity_df, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Generates user-based recommendations for a given user.\n",
    "\n",
    "    Args:\n",
    "        user_id (str): The ID of the target user.\n",
    "        user_item_matrix (pd.DataFrame): The user-item matrix.\n",
    "        user_similarity_df (pd.DataFrame): The user similarity matrix.\n",
    "        n_recommendations (int): The number of recommendations to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of recommended item IDs.\n",
    "    \"\"\"\n",
    "    if user_id not in user_similarity_df.index:\n",
    "        print(f\"User '{user_id}' not found in the similarity matrix.\")\n",
    "        return []\n",
    "\n",
    "    # Get the similarity scores for the target user\n",
    "    user_similarities = user_similarity_df.loc[user_id]\n",
    "\n",
    "    # Remove the user's own similarity score\n",
    "    user_similarities = user_similarities.drop(user_id)\n",
    "\n",
    "    # Sort similar users by similarity in descending order\n",
    "    similar_users = user_similarities.sort_values(ascending=False)\n",
    "\n",
    "    # Get items rated by the target user\n",
    "    items_rated_by_user = user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index\n",
    "\n",
    "    # Initialize a dictionary to store recommended item scores\n",
    "    item_scores = {}\n",
    "\n",
    "    # Iterate through similar users\n",
    "    for similar_user, similarity_score in similar_users.items():\n",
    "        if similarity_score <= 0: # Consider only users with positive similarity\n",
    "            continue\n",
    "\n",
    "        # Get items rated by the similar user\n",
    "        items_rated_by_similar_user = user_item_matrix.loc[similar_user][user_item_matrix.loc[similar_user] > 0].index\n",
    "\n",
    "        # Identify items rated by the similar user but not by the target user\n",
    "        items_to_consider = items_rated_by_similar_user.difference(items_rated_by_user)\n",
    "\n",
    "        # For each item, add the similar user's rating weighted by similarity\n",
    "        for item in items_to_consider:\n",
    "            if item not in item_scores:\n",
    "                item_scores[item] = 0\n",
    "            item_scores[item] += user_item_matrix.loc[similar_user, item] * similarity_score\n",
    "\n",
    "    # Sort items by their recommendation score in descending order\n",
    "    recommended_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top N recommended item IDs\n",
    "    return [item for item, score in recommended_items[:n_recommendations]]\n",
    "\n",
    "print(\"\\nUser-based recommendation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13f144bc"
   },
   "source": [
    "## Build item-based collaborative filtering model\n",
    "\n",
    "Implement an item-based collaborative filtering recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46961215"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate item similarity and define the item-based recommendation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "9cd60efb",
    "outputId": "84ddbf3a-affb-4001-cb6c-fbab8fd0aa80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Similarity Matrix head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. Fire File Chest</th>\n",
       "      <th>100:Complete First Season (blu-Ray)</th>\n",
       "      <th>2017-2018 Brownline174 Duraflex 14-Month Planner 8 1/2 X 11 Black</th>\n",
       "      <th>2x Ultra Era with Oxi Booster, 50fl oz</th>\n",
       "      <th>42 Dual Drop Leaf Table with 2 Madrid Chairs\"</th>\n",
       "      <th>4C Grated Parmesan Cheese 100% Natural 8oz Shaker</th>\n",
       "      <th>Africa's Best No-Lye Dual Conditioning Relaxer System Super</th>\n",
       "      <th>Alberto VO5 Salon Series Smooth Plus Sleek Shampoo</th>\n",
       "      <th>Alex Cross (dvdvideo)</th>\n",
       "      <th>All,bran Complete Wheat Flakes, 18 Oz.</th>\n",
       "      <th>...</th>\n",
       "      <th>Walkers Stem Ginger Shortbread</th>\n",
       "      <th>Wallmount Server Cabinet (450mm, 9 RU)</th>\n",
       "      <th>Way Basics 3-Shelf Eco Narrow Bookcase Storage Shelf, Espresso - Formaldehyde Free - Lifetime Guarantee</th>\n",
       "      <th>WeatherTech 40647 14-15 Outlander Cargo Liners Behind 2nd Row, Black</th>\n",
       "      <th>Wedding Wishes Wedding Guest Book</th>\n",
       "      <th>Weleda Everon Lip Balm</th>\n",
       "      <th>Wilton Black Dots Standard Baking Cups</th>\n",
       "      <th>Windex Original Glass Cleaner Refill 67.6oz (2 Liter)</th>\n",
       "      <th>Yes To Carrots Nourishing Body Wash</th>\n",
       "      <th>Yes To Grapefruit Rejuvenating Body Wash</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. Fire File Chest</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100:Complete First Season (blu-Ray)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-2018 Brownline174 Duraflex 14-Month Planner 8 1/2 X 11 Black</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2x Ultra Era with Oxi Booster, 50fl oz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42 Dual Drop Leaf Table with 2 Madrid Chairs\"</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146114</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "name                                                0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. Fire File Chest  \\\n",
       "name                                                                                                                \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                                1.0               \n",
       "100:Complete First Season (blu-Ray)                                                               0.0               \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                                0.0               \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                            0.0               \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                     0.0               \n",
       "\n",
       "name                                                100:Complete First Season (blu-Ray)  \\\n",
       "name                                                                                      \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                  0.0   \n",
       "100:Complete First Season (blu-Ray)                                                 1.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                  0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                              0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                       0.0   \n",
       "\n",
       "name                                                2017-2018 Brownline174 Duraflex 14-Month Planner 8 1/2 X 11 Black  \\\n",
       "name                                                                                                                    \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                                0.0                   \n",
       "100:Complete First Season (blu-Ray)                                                               0.0                   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                                1.0                   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                            0.0                   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                     0.0                   \n",
       "\n",
       "name                                                2x Ultra Era with Oxi Booster, 50fl oz  \\\n",
       "name                                                                                         \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                     0.0   \n",
       "100:Complete First Season (blu-Ray)                                                    0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                     0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                 1.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                          0.0   \n",
       "\n",
       "name                                                42 Dual Drop Leaf Table with 2 Madrid Chairs\"  \\\n",
       "name                                                                                                \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                            0.0   \n",
       "100:Complete First Season (blu-Ray)                                                           0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                            0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                        0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                 1.0   \n",
       "\n",
       "name                                                4C Grated Parmesan Cheese 100% Natural 8oz Shaker  \\\n",
       "name                                                                                                    \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                                0.0   \n",
       "100:Complete First Season (blu-Ray)                                                               0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                                0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                            0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                     0.0   \n",
       "\n",
       "name                                                Africa's Best No-Lye Dual Conditioning Relaxer System Super  \\\n",
       "name                                                                                                              \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                                0.0             \n",
       "100:Complete First Season (blu-Ray)                                                               0.0             \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                                0.0             \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                            0.0             \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                     0.0             \n",
       "\n",
       "name                                                Alberto VO5 Salon Series Smooth Plus Sleek Shampoo  \\\n",
       "name                                                                                                     \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                                0.0    \n",
       "100:Complete First Season (blu-Ray)                                                               0.0    \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                                0.0    \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                            0.0    \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                     0.0    \n",
       "\n",
       "name                                                Alex Cross (dvdvideo)  \\\n",
       "name                                                                        \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                    0.0   \n",
       "100:Complete First Season (blu-Ray)                                   0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                    0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                         0.0   \n",
       "\n",
       "name                                                All,bran Complete Wheat Flakes, 18 Oz.  \\\n",
       "name                                                                                         \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                     0.0   \n",
       "100:Complete First Season (blu-Ray)                                                    0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                     0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                 0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                          0.0   \n",
       "\n",
       "name                                                ...  \\\n",
       "name                                                ...   \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...  ...   \n",
       "100:Complete First Season (blu-Ray)                 ...   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...  ...   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz              ...   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"       ...   \n",
       "\n",
       "name                                                Walkers Stem Ginger Shortbread  \\\n",
       "name                                                                                 \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                             0.0   \n",
       "100:Complete First Season (blu-Ray)                                            0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                             0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                         0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                  0.0   \n",
       "\n",
       "name                                                Wallmount Server Cabinet (450mm, 9 RU)  \\\n",
       "name                                                                                         \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                     0.0   \n",
       "100:Complete First Season (blu-Ray)                                                    0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                     0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                 0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                          0.0   \n",
       "\n",
       "name                                                Way Basics 3-Shelf Eco Narrow Bookcase Storage Shelf, Espresso - Formaldehyde Free - Lifetime Guarantee  \\\n",
       "name                                                                                                                                                          \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                                0.0                                                         \n",
       "100:Complete First Season (blu-Ray)                                                               0.0                                                         \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                                0.0                                                         \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                            0.0                                                         \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                     0.0                                                         \n",
       "\n",
       "name                                                WeatherTech 40647 14-15 Outlander Cargo Liners Behind 2nd Row, Black  \\\n",
       "name                                                                                                                       \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                                0.0                      \n",
       "100:Complete First Season (blu-Ray)                                                               0.0                      \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                                0.0                      \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                            0.0                      \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                     0.0                      \n",
       "\n",
       "name                                                Wedding Wishes Wedding Guest Book  \\\n",
       "name                                                                                    \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                0.0   \n",
       "100:Complete First Season (blu-Ray)                                               0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                            0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                     0.0   \n",
       "\n",
       "name                                                Weleda Everon Lip Balm  \\\n",
       "name                                                                         \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                     0.0   \n",
       "100:Complete First Season (blu-Ray)                                    0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                     0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                 0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                          0.0   \n",
       "\n",
       "name                                                Wilton Black Dots Standard Baking Cups  \\\n",
       "name                                                                                         \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                     0.0   \n",
       "100:Complete First Season (blu-Ray)                                                    0.0   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                     0.0   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                 0.0   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                          0.0   \n",
       "\n",
       "name                                                Windex Original Glass Cleaner Refill 67.6oz (2 Liter)  \\\n",
       "name                                                                                                        \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                           0.000000       \n",
       "100:Complete First Season (blu-Ray)                                                          0.002118       \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                           0.000000       \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                       0.000000       \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                                0.000000       \n",
       "\n",
       "name                                                Yes To Carrots Nourishing Body Wash  \\\n",
       "name                                                                                      \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                             0.000000   \n",
       "100:Complete First Season (blu-Ray)                                            0.000000   \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                             0.000000   \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                         0.000000   \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                  0.146114   \n",
       "\n",
       "name                                                Yes To Grapefruit Rejuvenating Body Wash  \n",
       "name                                                                                          \n",
       "0.6 Cu. Ft. Letter A4 Size Waterproof 30 Min. F...                                       0.0  \n",
       "100:Complete First Season (blu-Ray)                                                      0.0  \n",
       "2017-2018 Brownline174 Duraflex 14-Month Planne...                                       0.0  \n",
       "2x Ultra Era with Oxi Booster, 50fl oz                                                   0.0  \n",
       "42 Dual Drop Leaf Table with 2 Madrid Chairs\"                                            0.0  \n",
       "\n",
       "[5 rows x 263 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item Similarity Matrix shape: (263, 263)\n",
      "\n",
      "Item-based recommendation function defined.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Calculate the pairwise cosine similarity between items (using the transposed user-item matrix)\n",
    "item_similarity = cosine_similarity(user_item_matrix.T)\n",
    "\n",
    "# 2. Convert the resulting similarity matrix into a pandas DataFrame\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
    "\n",
    "print(\"Item Similarity Matrix head:\")\n",
    "display(item_similarity_df.head())\n",
    "print(\"\\nItem Similarity Matrix shape:\", item_similarity_df.shape)\n",
    "\n",
    "# 3. Define a function item_based_recommendations\n",
    "def item_based_recommendations(item_name, user_item_matrix, item_similarity_df, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Generates item-based recommendations for a given item.\n",
    "\n",
    "    Args:\n",
    "        item_name (str): The name of the target item.\n",
    "        user_item_matrix (pd.DataFrame): The user-item matrix.\n",
    "        item_similarity_df (pd.DataFrame): The item similarity matrix.\n",
    "        n_recommendations (int): The number of recommendations to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of recommended item names.\n",
    "    \"\"\"\n",
    "    if item_name not in item_similarity_df.index:\n",
    "        print(f\"Item '{item_name}' not found in the similarity matrix.\")\n",
    "        return []\n",
    "\n",
    "    # 4. Get the similarity scores for the target item\n",
    "    item_similarities = item_similarity_df.loc[item_name]\n",
    "\n",
    "    # 5. Remove the item's own similarity score\n",
    "    item_similarities = item_similarities.drop(item_name, errors='ignore')\n",
    "\n",
    "    # 6. Sort similar items by similarity in descending order\n",
    "    similar_items = item_similarities.sort_values(ascending=False)\n",
    "\n",
    "    # 7. Get users who rated the target item\n",
    "    users_who_rated_item = user_item_matrix.index[user_item_matrix[item_name] > 0]\n",
    "\n",
    "    # 8. Initialize a dictionary to store recommended item scores\n",
    "    item_scores = {}\n",
    "\n",
    "    # 9. Iterate through similar items and users\n",
    "    for similar_item, similarity_score in similar_items.items():\n",
    "        if similarity_score <= 0: # Consider only items with positive similarity\n",
    "            continue\n",
    "\n",
    "        # 10. For each user, if they rated the similar item, add their rating weighted by the item similarity\n",
    "        for user in users_who_rated_item:\n",
    "            if user_item_matrix.loc[user, similar_item] > 0:\n",
    "                if similar_item not in item_scores:\n",
    "                    item_scores[similar_item] = 0\n",
    "                item_scores[similar_item] += user_item_matrix.loc[user, similar_item] * similarity_score\n",
    "\n",
    "    # 11. Sort items in the item_scores dictionary by their recommendation score\n",
    "    recommended_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 12. Return the top N recommended item names\n",
    "    return [item for item, score in recommended_items[:n_recommendations]]\n",
    "\n",
    "print(\"\\nItem-based recommendation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8025b86"
   },
   "source": [
    "## Evaluate recommendation systems\n",
    "\n",
    "Evaluate the performance of both the user-based and item-based recommendation systems using appropriate metrics (e.g., RMSE, precision, recall).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e020218a"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the evaluation function for the recommendation systems and then call it for both user-based and item-based models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "d78a7068",
    "outputId": "ce0636ce-bc9f-4628-a739-4bccf740f590"
   },
   "outputs": [],
   "source": [
    "def evaluate_recommendation_system(test_df, recommendation_function, user_item_matrix, similarity_matrix, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a recommendation system.\n",
    "\n",
    "    Args:\n",
    "        test_df (pd.DataFrame): DataFrame containing the test set (user, item, rating).\n",
    "        recommendation_function (function): The function to generate recommendations.\n",
    "        user_item_matrix (pd.DataFrame): The user-item matrix from the training data.\n",
    "        similarity_matrix (pd.DataFrame): The user or item similarity matrix.\n",
    "        n_recommendations (int): The number of recommendations generated by the system.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics (e.g., hit rate).\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    total_test_interactions = len(test_df)\n",
    "    users_in_train = user_item_matrix.index.tolist()\n",
    "    items_in_train = user_item_matrix.columns.tolist()\n",
    "\n",
    "    for index, row in tqdm(test_df.iterrows(), total=total_test_interactions, desc=f\"Evaluating {recommendation_function.__name__}\"):\n",
    "        user = row['reviews_username']\n",
    "        actual_item = row['name']\n",
    "\n",
    "        # Only evaluate if the user and item are in the training data's matrix\n",
    "        # This is a limitation of collaborative filtering - it cannot recommend for new users/items\n",
    "        if user in users_in_train and actual_item in items_in_train:\n",
    "            # Generate recommendations for the user\n",
    "            if recommendation_function.__name__ == 'user_based_recommendations':\n",
    "                 recommended_items = recommendation_function(user, user_item_matrix, similarity_matrix, n_recommendations)\n",
    "            elif recommendation_function.__name__ == 'item_based_recommendations':\n",
    "                 # For item-based, we need to provide an item from the user's history in the training set\n",
    "                 # This is a simplification; a real system would use all items rated by the user\n",
    "                 user_rated_items_in_train = user_item_matrix.loc[user][user_item_matrix.loc[user] > 0].index.tolist()\n",
    "                 if not user_rated_items_in_train: # Skip if user has no rated items in training (should be handled by user_in_train check, but double-checking)\n",
    "                     continue\n",
    "                 # Use the first item the user rated in the training set as a basis for item-based recommendation\n",
    "                 # A more sophisticated approach would aggregate recommendations from all rated items\n",
    "                 seed_item = user_rated_items_in_train[0]\n",
    "                 recommended_items = recommendation_function(seed_item, user_item_matrix, similarity_matrix, n_recommendations)\n",
    "            else:\n",
    "                print(f\"Unknown recommendation function: {recommendation_function.__name__}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Check if the actual item is in the recommendations\n",
    "            if actual_item in recommended_items:\n",
    "                hits += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    hit_rate = hits / total_test_interactions if total_test_interactions > 0 else 0\n",
    "\n",
    "    return {\"Hit Rate\": hit_rate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating User-Based Recommendation System...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating user_based_recommendations: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5989/5989 [04:35<00:00, 21.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based Metrics: {'Hit Rate': 0.07697445316413425}\n",
      "\n",
      "Evaluating Item-Based Recommendation System...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating item_based_recommendations: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5989/5989 [12:14<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-Based Metrics: {'Hit Rate': 0.08281850058440474}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate User-Based Recommendation System\n",
    "print(\"Evaluating User-Based Recommendation System...\")\n",
    "user_based_metrics = evaluate_recommendation_system(test_df, user_based_recommendations, user_item_matrix, user_similarity_df)\n",
    "print(\"User-Based Metrics:\", user_based_metrics)\n",
    "\n",
    "# Evaluate Item-Based Recommendation System\n",
    "print(\"\\nEvaluating Item-Based Recommendation System...\")\n",
    "item_based_metrics = evaluate_recommendation_system(test_df, item_based_recommendations, user_item_matrix, item_similarity_df)\n",
    "print(\"Item-Based Metrics:\", item_based_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89678842"
   },
   "source": [
    "**Reasoning**:\n",
    "The evaluation of the item-based recommendation system is complete, print the metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb4f9f5e",
    "outputId": "d042e62b-9699-4f7c-e34a-add580875a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-Based Metrics: {'Hit Rate': 0.08281850058440474}\n"
     ]
    }
   ],
   "source": [
    "print(\"Item-Based Metrics:\", item_based_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc5882a4"
   },
   "source": [
    "## Compare and select the best model\n",
    "\n",
    "Compare the performance of the two recommendation systems and select the best one based on the evaluation results, providing reasons for the selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbb7f888"
   },
   "source": [
    "**Reasoning**:\n",
    "Compare the evaluation metrics and select the best performing model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd021995",
    "outputId": "b950e834-2ad5-45eb-c5a5-80ca14308d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based Metrics: {'Hit Rate': 0.07697445316413425}\n",
      "Item-Based Metrics: {'Hit Rate': 0.08281850058440474}\n",
      "\n",
      "Best Performing Recommendation Model: Item-Based Collaborative Filtering\n",
      "Reason for Selection: The Item-Based model achieved a higher Hit Rate (0.0828) compared to the User-Based model (0.0770).\n"
     ]
    }
   ],
   "source": [
    "print(\"User-Based Metrics:\", user_based_metrics)\n",
    "print(\"Item-Based Metrics:\", item_based_metrics)\n",
    "\n",
    "# Compare the Hit Rates and select the best model\n",
    "if item_based_metrics['Hit Rate'] > user_based_metrics['Hit Rate']:\n",
    "    best_model = \"Item-Based Collaborative Filtering\"\n",
    "    best_metric_value = item_based_metrics['Hit Rate']\n",
    "    reason = f\"The Item-Based model achieved a higher Hit Rate ({best_metric_value:.4f}) compared to the User-Based model ({user_based_metrics['Hit Rate']:.4f}).\"\n",
    "elif user_based_metrics['Hit Rate'] > item_based_metrics['Hit Rate']:\n",
    "    best_model = \"User-Based Collaborative Filtering\"\n",
    "    best_metric_value = user_based_metrics['Hit Rate']\n",
    "    reason = f\"The User-Based model achieved a higher Hit Rate ({best_metric_value:.4f}) compared to the Item-Based model ({item_based_metrics['Hit Rate']:.4f}).\"\n",
    "else:\n",
    "    best_model = \"Both models performed equally\"\n",
    "    best_metric_value = user_based_metrics['Hit Rate'] # or item_based_metrics['Hit Rate']\n",
    "    reason = f\"Both models achieved the same Hit Rate ({best_metric_value:.4f}).\"\n",
    "\n",
    "print(f\"\\nBest Performing Recommendation Model: {best_model}\")\n",
    "print(f\"Reason for Selection: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c06660d"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   The user-item matrix created from the training data contained 20527 unique users and 263 unique items.\n",
    "*   The user-based recommendation system achieved a Hit Rate of approximately 0.0770 on the test set.\n",
    "*   The item-based recommendation system achieved a Hit Rate of approximately 0.0828 on the test set, slightly outperforming the user-based model.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   The item-based collaborative filtering model is selected as the best performing model due to its higher Hit Rate on the test data.\n",
    "*   Further improvements could involve exploring different similarity metrics, incorporating regularization techniques, or utilizing hybrid approaches combining content-based or model-based methods to address the limitations of collaborative filtering (e.g., cold start problem).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP3TVLMyK3US",
    "outputId": "4c7abb9d-5d7e-4f95-b367-ed5d6170d0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'Clorox Disinfecting Wipes Value Pack Scented 150 Ct Total' as the seed item for recommendations for user 'rebecca'.\n",
      "\n",
      "Top 20 Recommended Items for user 'rebecca':\n",
      "1. Clorox Disinfecting Bathroom Cleaner\n",
      "2. Godzilla 3d Includes Digital Copy Ultraviolet 3d/2d Blu-Ray/dvd\n",
      "3. Tostitos Bite Size Tortilla Chips\n",
      "4. Burt's Bees Lip Shimmer, Raisin\n",
      "5. My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Digital)\n",
      "6. Mike Dave Need Wedding Dates (dvd + Digital)\n",
      "7. Planes: Fire Rescue (2 Discs) (includes Digital Copy) (blu-Ray/dvd)\n",
      "8. Coty Airspun Face Powder, Translucent Extra Coverage\n",
      "9. Chex Muddy Buddies Brownie Supreme Snack Mix\n",
      "10. Pendaflex174 Divide It Up File Folder, Multi Section, Letter, Assorted, 12/pack\n",
      "11. Chips Deluxe Soft 'n Chewy Cookies\n",
      "12. The Resident Evil Collection 5 Discs (blu-Ray)\n",
      "13. The Script - No Sound Without Silence (cd)\n",
      "14. Vicks Vaporub, Regular, 3.53oz\n",
      "15. Hormel Chili, No Beans\n",
      "16. Chester's Cheese Flavored Puffcorn Snacks\n",
      "17. Vaseline Intensive Care Lip Therapy Cocoa Butter\n",
      "18. Stander Pt Bedcane - Bed Handle\n",
      "19. Nexxus Exxtra Gel Style Creation Sculptor\n",
      "20. 100:Complete First Season (blu-Ray)\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Generate recommendations for a user using the best model\n",
    "\n",
    "# Specify the username for whom you want recommendations\n",
    "# Replace 'Enter_Username_Here' with the actual username from your dataset\n",
    "target_username = 'rebecca' # Example username, replace with a user from your dataset\n",
    "\n",
    "# Check if the target user exists in the user-item matrix\n",
    "if target_username not in user_item_matrix.index:\n",
    "    print(f\"User '{target_username}' not found in the training data.\")\n",
    "else:\n",
    "    # To use the item-based recommendation function, we need an item the user has rated\n",
    "    # We can pick one of the items the user rated in the training set as a seed\n",
    "    user_rated_items_in_train = user_item_matrix.loc[target_username][user_item_matrix.loc[target_username] > 0].index.tolist()\n",
    "\n",
    "    if not user_rated_items_in_train:\n",
    "        print(f\"User '{target_username}' has not rated any items in the training data. Cannot generate item-based recommendations.\")\n",
    "    else:\n",
    "        # Use the first item the user rated in the training set as the seed item\n",
    "        seed_item_for_recommendation = user_rated_items_in_train[0]\n",
    "        print(f\"Using '{seed_item_for_recommendation}' as the seed item for recommendations for user '{target_username}'.\")\n",
    "\n",
    "        # Generate top N recommendations using the item-based model\n",
    "        n_recommendations = 20\n",
    "        recommended_items = item_based_recommendations(seed_item_for_recommendation, user_item_matrix, item_similarity_df, n_recommendations)\n",
    "\n",
    "        if recommended_items:\n",
    "            print(f\"\\nTop {n_recommendations} Recommended Items for user '{target_username}':\")\n",
    "            for i, item in enumerate(recommended_items):\n",
    "                print(f\"{i+1}. {item}\")\n",
    "        else:\n",
    "            print(f\"Could not generate recommendations for user '{target_username}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "298139b6"
   },
   "source": [
    "\n",
    "Analyze the reviews of the top 20 recommended products for a user, predict the sentiment of these reviews using the best performing sentiment analysis model, calculate the percentage of positive sentiments for each of the 20 products, and identify the top 5 products with the highest percentage of positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d574e9c"
   },
   "source": [
    "## Filter reviews for recommended products\n",
    "\n",
    "Create a DataFrame containing only the reviews for the top 20 recommended products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27e3a1ca"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a DataFrame containing only the reviews for the top 20 recommended products.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "9dd81e6e",
    "outputId": "1128c6c8-bf1f-458a-e0e8-76cc3eb6dfdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame head with reviews for top 20 recommended products:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>name</th>\n",
       "      <th>reviews_date</th>\n",
       "      <th>reviews_didPurchase</th>\n",
       "      <th>reviews_doRecommend</th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>reviews_title</th>\n",
       "      <th>reviews_username</th>\n",
       "      <th>user_sentiment</th>\n",
       "      <th>reviews_text_preprocessed</th>\n",
       "      <th>reviews_title_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>AVpe41TqilAPnD_xQH3d</td>\n",
       "      <td>FOX</td>\n",
       "      <td>Movies &amp; TV Shows,Movies,Romance,Romantic Come...</td>\n",
       "      <td>Mike Dave Need Wedding Dates (dvd + Digital)</td>\n",
       "      <td>2016-10-02 00:00:00+00:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>I expected more from this movie and more from ...</td>\n",
       "      <td>Not as funny as I thought</td>\n",
       "      <td>elite</td>\n",
       "      <td>Positive</td>\n",
       "      <td>expected movie zac enron would give movie try ...</td>\n",
       "      <td>funny thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>AVpe41TqilAPnD_xQH3d</td>\n",
       "      <td>FOX</td>\n",
       "      <td>Movies &amp; TV Shows,Movies,Romance,Romantic Come...</td>\n",
       "      <td>Mike Dave Need Wedding Dates (dvd + Digital)</td>\n",
       "      <td>2016-11-13 00:00:00+00:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Would ABSOLUTELY NOT recommend. We could only ...</td>\n",
       "      <td>Mike &amp; Dave Need Wedding Dates</td>\n",
       "      <td>tampa</td>\n",
       "      <td>Negative</td>\n",
       "      <td>would absolutely recommend could take 15 minut...</td>\n",
       "      <td>mike dave need wedding date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>AVpe41TqilAPnD_xQH3d</td>\n",
       "      <td>FOX</td>\n",
       "      <td>Movies &amp; TV Shows,Movies,Romance,Romantic Come...</td>\n",
       "      <td>Mike Dave Need Wedding Dates (dvd + Digital)</td>\n",
       "      <td>2016-12-03 00:00:00+00:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible movie with good actors. Can't believe...</td>\n",
       "      <td>Horrible movie</td>\n",
       "      <td>johnny</td>\n",
       "      <td>Negative</td>\n",
       "      <td>terrible movie good actor cant believe spent 2...</td>\n",
       "      <td>horrible movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>AVpe41TqilAPnD_xQH3d</td>\n",
       "      <td>FOX</td>\n",
       "      <td>Movies &amp; TV Shows,Movies,Romance,Romantic Come...</td>\n",
       "      <td>Mike Dave Need Wedding Dates (dvd + Digital)</td>\n",
       "      <td>2016-12-23 00:00:00+00:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>This movie is terrible with only a few funny s...</td>\n",
       "      <td>Terrible</td>\n",
       "      <td>raiderfan1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>movie terrible funny scene</td>\n",
       "      <td>terrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>AVpe41TqilAPnD_xQH3d</td>\n",
       "      <td>FOX</td>\n",
       "      <td>Movies &amp; TV Shows,Movies,Romance,Romantic Come...</td>\n",
       "      <td>Mike Dave Need Wedding Dates (dvd + Digital)</td>\n",
       "      <td>2017-01-06 00:00:00+00:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>This was a boring movie i couldnt watch 20 min...</td>\n",
       "      <td>Mike</td>\n",
       "      <td>viktoorhdz</td>\n",
       "      <td>Negative</td>\n",
       "      <td>boring movie couldnt watch 20 minute</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id brand  \\\n",
       "1796  AVpe41TqilAPnD_xQH3d   FOX   \n",
       "1797  AVpe41TqilAPnD_xQH3d   FOX   \n",
       "1798  AVpe41TqilAPnD_xQH3d   FOX   \n",
       "1799  AVpe41TqilAPnD_xQH3d   FOX   \n",
       "1800  AVpe41TqilAPnD_xQH3d   FOX   \n",
       "\n",
       "                                             categories  \\\n",
       "1796  Movies & TV Shows,Movies,Romance,Romantic Come...   \n",
       "1797  Movies & TV Shows,Movies,Romance,Romantic Come...   \n",
       "1798  Movies & TV Shows,Movies,Romance,Romantic Come...   \n",
       "1799  Movies & TV Shows,Movies,Romance,Romantic Come...   \n",
       "1800  Movies & TV Shows,Movies,Romance,Romantic Come...   \n",
       "\n",
       "                                              name              reviews_date  \\\n",
       "1796  Mike Dave Need Wedding Dates (dvd + Digital) 2016-10-02 00:00:00+00:00   \n",
       "1797  Mike Dave Need Wedding Dates (dvd + Digital) 2016-11-13 00:00:00+00:00   \n",
       "1798  Mike Dave Need Wedding Dates (dvd + Digital) 2016-12-03 00:00:00+00:00   \n",
       "1799  Mike Dave Need Wedding Dates (dvd + Digital) 2016-12-23 00:00:00+00:00   \n",
       "1800  Mike Dave Need Wedding Dates (dvd + Digital) 2017-01-06 00:00:00+00:00   \n",
       "\n",
       "     reviews_didPurchase reviews_doRecommend  reviews_rating  \\\n",
       "1796             Unknown               False               1   \n",
       "1797             Unknown               False               1   \n",
       "1798             Unknown               False               1   \n",
       "1799             Unknown               False               1   \n",
       "1800             Unknown               False               1   \n",
       "\n",
       "                                           reviews_text  \\\n",
       "1796  I expected more from this movie and more from ...   \n",
       "1797  Would ABSOLUTELY NOT recommend. We could only ...   \n",
       "1798  Terrible movie with good actors. Can't believe...   \n",
       "1799  This movie is terrible with only a few funny s...   \n",
       "1800  This was a boring movie i couldnt watch 20 min...   \n",
       "\n",
       "                       reviews_title reviews_username user_sentiment  \\\n",
       "1796       Not as funny as I thought            elite       Positive   \n",
       "1797  Mike & Dave Need Wedding Dates            tampa       Negative   \n",
       "1798                  Horrible movie           johnny       Negative   \n",
       "1799                        Terrible       raiderfan1       Negative   \n",
       "1800                            Mike       viktoorhdz       Negative   \n",
       "\n",
       "                              reviews_text_preprocessed  \\\n",
       "1796  expected movie zac enron would give movie try ...   \n",
       "1797  would absolutely recommend could take 15 minut...   \n",
       "1798  terrible movie good actor cant believe spent 2...   \n",
       "1799                         movie terrible funny scene   \n",
       "1800               boring movie couldnt watch 20 minute   \n",
       "\n",
       "       reviews_title_preprocessed  \n",
       "1796                funny thought  \n",
       "1797  mike dave need wedding date  \n",
       "1798               horrible movie  \n",
       "1799                     terrible  \n",
       "1800                         mike  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the filtered DataFrame: (11924, 14)\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a list named top_20_recommended_items\n",
    "top_20_recommended_items = recommended_items\n",
    "\n",
    "# 2. Filter the original DataFrame df\n",
    "df_recommended_reviews = df[df['name'].isin(top_20_recommended_items)]\n",
    "\n",
    "# 3. Display the head and the shape of the df_recommended_reviews DataFrame\n",
    "print(\"DataFrame head with reviews for top 20 recommended products:\")\n",
    "display(df_recommended_reviews.head())\n",
    "print(\"\\nShape of the filtered DataFrame:\", df_recommended_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a8bc576"
   },
   "source": [
    "## Predict sentiment for recommended products' reviews\n",
    "\n",
    "Use the best sentiment analysis model (Random Forest, based on previous evaluation) to predict the sentiment (positive or negative) for each review of the recommended products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22697c28"
   },
   "source": [
    "**Reasoning**:\n",
    "Use the best sentiment analysis model to predict the sentiment of the reviews for the recommended products.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "7f9d98e5",
    "outputId": "908c40e4-309b-433f-df56-2f88341d52a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame head with predicted sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "/var/folders/mr/wqzk53n960gcw6f9pdl5gzg80000gp/T/ipykernel_85678/2789633806.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_recommended_reviews['predicted_sentiment'] = predicted_sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>reviews_text_preprocessed</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>I expected more from this movie and more from ...</td>\n",
       "      <td>expected movie zac enron would give movie try ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>Would ABSOLUTELY NOT recommend. We could only ...</td>\n",
       "      <td>would absolutely recommend could take 15 minut...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>Terrible movie with good actors. Can't believe...</td>\n",
       "      <td>terrible movie good actor cant believe spent 2...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>This movie is terrible with only a few funny s...</td>\n",
       "      <td>movie terrible funny scene</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>This was a boring movie i couldnt watch 20 min...</td>\n",
       "      <td>boring movie couldnt watch 20 minute</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reviews_text  \\\n",
       "1796  I expected more from this movie and more from ...   \n",
       "1797  Would ABSOLUTELY NOT recommend. We could only ...   \n",
       "1798  Terrible movie with good actors. Can't believe...   \n",
       "1799  This movie is terrible with only a few funny s...   \n",
       "1800  This was a boring movie i couldnt watch 20 min...   \n",
       "\n",
       "                              reviews_text_preprocessed predicted_sentiment  \n",
       "1796  expected movie zac enron would give movie try ...            Positive  \n",
       "1797  would absolutely recommend could take 15 minut...            Negative  \n",
       "1798  terrible movie good actor cant believe spent 2...            Negative  \n",
       "1799                         movie terrible funny scene            Negative  \n",
       "1800               boring movie couldnt watch 20 minute            Negative  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Select the 'reviews_text_preprocessed' column from the df_recommended_reviews DataFrame\n",
    "X_recommended = df_recommended_reviews['reviews_text_preprocessed']\n",
    "\n",
    "# 2. Use the fitted TF-IDF vectorizer to transform the preprocessed text data\n",
    "X_recommended_tfidf = tfidf_vectorizer.transform(X_recommended)\n",
    "\n",
    "# 3. Use the best performing sentiment analysis model (Random Forest) to predict the sentiment labels\n",
    "# The best model was identified as Random Forest (rf_clf) in the previous sentiment analysis task.\n",
    "predicted_sentiment = rf_clf.predict(X_recommended_tfidf)\n",
    "\n",
    "# 4. Add the predicted sentiment labels as a new column named 'predicted_sentiment' to the df_recommended_reviews DataFrame\n",
    "df_recommended_reviews['predicted_sentiment'] = predicted_sentiment\n",
    "\n",
    "# 5. Display the head of the df_recommended_reviews DataFrame\n",
    "print(\"\\nDataFrame head with predicted sentiment:\")\n",
    "display(df_recommended_reviews[['reviews_text', 'reviews_text_preprocessed', 'predicted_sentiment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6e0b81c"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate the percentage of positive sentiments for each recommended product and identify the top 5 products with the highest percentage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WO4DUgPWLvW0",
    "outputId": "84f4ad81-f38f-4b0c-a8de-cde80797ed74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of Positive Sentiment for Recommended Products (Sorted):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name\n",
       "Chips Deluxe Soft 'n Chewy Cookies                                                 100.000000\n",
       "My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Digital)                                95.808383\n",
       "100:Complete First Season (blu-Ray)                                                 94.244604\n",
       "Planes: Fire Rescue (2 Discs) (includes Digital Copy) (blu-Ray/dvd)                 93.432574\n",
       "Clorox Disinfecting Bathroom Cleaner                                                92.447278\n",
       "Godzilla 3d Includes Digital Copy Ultraviolet 3d/2d Blu-Ray/dvd                     92.090226\n",
       "Coty Airspun Face Powder, Translucent Extra Coverage                                89.240506\n",
       "Vaseline Intensive Care Lip Therapy Cocoa Butter                                    88.607595\n",
       "Burt's Bees Lip Shimmer, Raisin                                                     88.087056\n",
       "The Script - No Sound Without Silence (cd)                                          87.179487\n",
       "Chex Muddy Buddies Brownie Supreme Snack Mix                                        86.206897\n",
       "Mike Dave Need Wedding Dates (dvd + Digital)                                        85.072655\n",
       "Stander Pt Bedcane - Bed Handle                                                     84.615385\n",
       "The Resident Evil Collection 5 Discs (blu-Ray)                                      81.301775\n",
       "Pendaflex174 Divide It Up File Folder, Multi Section, Letter, Assorted, 12/pack     80.322581\n",
       "Tostitos Bite Size Tortilla Chips                                                   79.166667\n",
       "Hormel Chili, No Beans                                                              78.571429\n",
       "Chester's Cheese Flavored Puffcorn Snacks                                           76.271186\n",
       "Vicks Vaporub, Regular, 3.53oz                                                      62.500000\n",
       "Nexxus Exxtra Gel Style Creation Sculptor                                           57.864358\n",
       "Name: predicted_sentiment, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Products with Highest Percentage of Positive Reviews:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name\n",
       "Chips Deluxe Soft 'n Chewy Cookies                                     100.000000\n",
       "My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Digital)                    95.808383\n",
       "100:Complete First Season (blu-Ray)                                     94.244604\n",
       "Planes: Fire Rescue (2 Discs) (includes Digital Copy) (blu-Ray/dvd)     93.432574\n",
       "Clorox Disinfecting Bathroom Cleaner                                    92.447278\n",
       "Name: predicted_sentiment, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the percentage of positive sentiments for each product\n",
    "positive_sentiment_percentage = df_recommended_reviews.groupby('name')['predicted_sentiment'].apply(lambda x: (x == 'Positive').sum() / len(x) * 100)\n",
    "\n",
    "# Sort the products by the percentage of positive sentiment in descending order\n",
    "sorted_positive_sentiment = positive_sentiment_percentage.sort_values(ascending=False)\n",
    "\n",
    "# Identify the top 5 products with the highest percentage of positive reviews\n",
    "top_5_positive_products = sorted_positive_sentiment.head(5)\n",
    "\n",
    "# Display the percentage of positive sentiment for all recommended products (sorted)\n",
    "print(\"\\nPercentage of Positive Sentiment for Recommended Products (Sorted):\")\n",
    "display(sorted_positive_sentiment)\n",
    "\n",
    "# Display the top 5 products with the highest percentage of positive reviews\n",
    "print(\"\\nTop 5 Products with Highest Percentage of Positive Reviews:\")\n",
    "display(top_5_positive_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b266cb6"
   },
   "source": [
    "## Present the top 5 products\n",
    "\n",
    "Present the names of the top 5 products with the highest percentage of positive reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0ea3162"
   },
   "source": [
    "**Reasoning**:\n",
    "Print the heading and iterate through the top 5 positive products to display their rank, name, and positive sentiment percentage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2053dfa4",
    "outputId": "3c8efcbb-8771-4b49-c049-152f46716d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Products with Highest Percentage of Positive Reviews:\n",
      "1. Chips Deluxe Soft 'n Chewy Cookies: 100.00% Positive\n",
      "2. My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Digital): 95.81% Positive\n",
      "3. 100:Complete First Season (blu-Ray): 94.24% Positive\n",
      "4. Planes: Fire Rescue (2 Discs) (includes Digital Copy) (blu-Ray/dvd): 93.43% Positive\n",
      "5. Clorox Disinfecting Bathroom Cleaner: 92.45% Positive\n"
     ]
    }
   ],
   "source": [
    "# Print a clear heading\n",
    "print(\"Top 5 Products with Highest Percentage of Positive Reviews:\")\n",
    "\n",
    "# Iterate through the top_5_positive_products Series and print the results\n",
    "for rank, (product_name, percentage) in enumerate(top_5_positive_products.items()):\n",
    "    print(f\"{rank + 1}. {product_name}: {percentage:.2f}% Positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75b08d6f"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   There are 11924 reviews available for the top 20 recommended products.\n",
    "*   The Random Forest model was used to predict the sentiment of these reviews, categorizing them as either 'Positive' or 'Negative'.\n",
    "*   The percentage of positive sentiment varies among the recommended products.\n",
    "*   \"Chips Deluxe Soft 'n Chewy Cookies\" has the highest percentage of positive reviews at 100%.\n",
    "*   The top 5 products with the highest percentage of positive reviews are:\n",
    "    1.  Chips Deluxe Soft 'n Chewy Cookies: 100.00% Positive\n",
    "    2.  My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Digital): 95.66% Positive\n",
    "    3.  100:Complete First Season (blu-Ray): 94.24% Positive\n",
    "    4.  Planes: Fire Rescue (2 Discs) (includes Digital Copy) (blu-Ray/dvd): 93.26% Positive\n",
    "    5.  Clorox Disinfecting Bathroom Cleaner: 92.45% Positive\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   The high percentage of positive reviews for the top 5 products suggests these items are likely to be well-received by users. Consider highlighting these products in marketing or promotional efforts.\n",
    "*   Further analysis could involve examining the content of the negative reviews for the other recommended products to identify areas for potential improvement or address common complaints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89805d9f"
   },
   "source": [
    "\n",
    "Lets generate pickel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYI1OuH67Teb",
    "outputId": "91e129d8-401a-4914-eda5-4eaca69ee7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved user_item_matrix to recommendation_app/pickles/user_item_matrix.pkl\n",
      "Saved item_similarity_df to recommendation_app/pickles/item_similarity_df.pkl\n",
      "Saved rf_clf to recommendation_app/pickles/rf_clf.pkl\n",
      "Saved tfidf_vectorizer to recommendation_app/pickles/tfidf_vectorizer.pkl\n",
      "Saved df to recommendation_app/pickles/df.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create a directory to save the pickle files\n",
    "pickle_dir = 'recommendation_app/pickles'\n",
    "os.makedirs(pickle_dir, exist_ok=True)\n",
    "\n",
    "# Define the file paths\n",
    "user_item_matrix_path = os.path.join(pickle_dir, 'user_item_matrix.pkl')\n",
    "item_similarity_df_path = os.path.join(pickle_dir, 'item_similarity_df.pkl')\n",
    "rf_clf_path = os.path.join(pickle_dir, 'rf_clf.pkl')\n",
    "tfidf_vectorizer_path = os.path.join(pickle_dir, 'tfidf_vectorizer.pkl')\n",
    "df_path = os.path.join(pickle_dir, 'df.pkl')\n",
    "\n",
    "# Save the variables using pickle\n",
    "try:\n",
    "    with open(user_item_matrix_path, 'wb') as f:\n",
    "        pickle.dump(user_item_matrix, f)\n",
    "    print(f\"Saved user_item_matrix to {user_item_matrix_path}\")\n",
    "\n",
    "    with open(item_similarity_df_path, 'wb') as f:\n",
    "        pickle.dump(item_similarity_df, f)\n",
    "    print(f\"Saved item_similarity_df to {item_similarity_df_path}\")\n",
    "\n",
    "    with open(rf_clf_path, 'wb') as f:\n",
    "        pickle.dump(rf_clf, f)\n",
    "    print(f\"Saved rf_clf to {rf_clf_path}\")\n",
    "\n",
    "    with open(tfidf_vectorizer_path, 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    print(f\"Saved tfidf_vectorizer to {tfidf_vectorizer_path}\")\n",
    "\n",
    "    with open(df_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    print(f\"Saved df to {df_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0slNjZ-FzxPa"
   },
   "source": [
    "**Github Link For Repo :-** https://github.com/RohitKini/CapstoneProject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inyH8Qanz_K8"
   },
   "source": [
    "**App Deployed on Heroku Link :-**\n",
    "\n",
    "https://product-recommendation-project-8a76b78d452d.herokuapp.com/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Capstone (.venv)",
   "language": "python",
   "name": "capstone-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
